!pip install -U huggingface_hub

from huggingface_hub import InferenceClient
import os

# ðŸ”‘ Insert your Hugging Face API token here
HF_TOKEN = "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"  # Replace with your actual token

# Set up client with an instruction-tuned model (like Mistral or Falcon)
client = InferenceClient(model="mistralai/Mistral-7B-Instruct-v0.1", token=HF_TOKEN)

# ... (rest of your code) ...


chat_history = []

while True:
    query = input("You: ")
    if query.lower() in ["exit", "quit"]:
        print("ðŸ‘‹ Exiting chatbot.")
        break

    # Build prompt including chat history
    history_text = ""
    for user, bot in chat_history:
        history_text += f"User: {user}\nBot: {bot}\n"
    prompt = f"{history_text}User: {query}\nBot:"

    # Call Hugging Face model
    response = client.text_generation(
        prompt,
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True,
    )

    answer = response.strip()
    print("ðŸ¤– Bot:", answer)
    chat_history.append((query, answer))
